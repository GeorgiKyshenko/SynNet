{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VgqajlO_yAZN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade langchain_core\n",
        "!pip install --upgrade langchain_community\n",
        "!pip install --upgrade langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gBTHZb6bAio0"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0io2-K2pQPvL"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjPY8e_3aDsD"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1aVu-F0Kcq9z"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.docstore.document import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.llms import LlamaCpp\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSx4WZ1xzalH"
      },
      "source": [
        "#### First Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK7kojmbys71",
        "outputId": "ad795b0c-4132-40a6-e419-907660fa78d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Malaria is a serious and sometimes fatal disease caused by a parasite that commonly infects a certain type of mosquito, which then feeds on humans.\n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "1.  **Cause:** It is caused by Plasmodium parasites. There are five species that infect humans, with *Plasmodium falciparum* being the most dangerous and responsible for most malaria-related deaths worldwide.\n",
            "2.  **Transmission:** Malaria is transmitted through the bite of an infected female *Anopheles* mosquito. When an infected mosquito bites a person, it injects the parasites into the bloodstream.\n",
            "3.  **Life Cycle:** Once in the human body, the parasites travel to the liver, where they mature and multiply. After a period, they leave the liver and infect red blood cells, where they continue to multiply, causing the red blood cells to burst. This cycle leads to the characteristic symptoms of malaria.\n",
            "4.  **Symptoms:** Symptoms typically appear 10 days to 4 weeks after infection, though they can appear as early as 7 days or as late as several months. Common symptoms include:\n",
            "    *   High fever\n",
            "    *   Chills\n",
            "    *   Headache\n",
            "    *   Muscle aches\n",
            "    *   Fatigue\n",
            "    *   Nausea and vomiting\n",
            "    *   Diarrhea\n",
            "    *   Sweating\n",
            "    *   Anemia and jaundice (yellowing of skin and eyes) can also occur.\n",
            "    The symptoms often occur in cycles, alternating between periods of chills, fever, and sweating.\n",
            "5.  **Complications:** If not promptly diagnosed and treated, malaria can lead to severe complications, especially with *P. falciparum* infection. These can include:\n",
            "    *   Cerebral malaria (seizures, coma)\n",
            "    *   Severe anemia\n",
            "    *   Acute kidney failure\n",
            "    *   Acute respiratory distress syndrome (ARDS)\n",
            "    *   Low blood sugar (hypoglycemia)\n",
            "    *   Circulatory collapse\n",
            "    These complications can be life-threatening.\n",
            "6.  **Treatment:** Malaria is a treatable disease, especially when diagnosed early. Treatment involves antimalarial drugs, which vary depending on the type of malaria, the severity of the illness, and the patient's age and other health conditions.\n",
            "7.  **Prevention:** Prevention strategies include:\n",
            "    *   **Mosquito bite prevention:** Using insect repellent, wearing long sleeves and pants, sleeping under insecticide-treated bed nets, and ensuring screens on windows and doors.\n",
            "    *   **Antimalarial medications (prophylaxis):** Taken before, during, and after travel to malaria-endemic areas.\n",
            "    *   **Vaccines:** The RTS,S/AS01 (Mosquirix) vaccine is the first and only malaria vaccine recommended by the WHO for widespread use in children in areas with moderate to high *P. falciparum* malaria transmission.\n",
            "\n",
            "Malaria remains a major global health problem, particularly in sub-Saharan Africa, where it causes hundreds of thousands of deaths each year, predominantly among young children.\n"
          ]
        }
      ],
      "source": [
        "# 1. Initializing the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0, google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Prepare the messages\n",
        "# The SystemMessage sets the behavior and context for the AI (If i want normal behavior without restriction, i can skip the SystemMessage)\n",
        "# The HumanMessage is the user`s actual query\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"What is Malaria?\")\n",
        "]\n",
        "\n",
        "# 3. Invoke the model with the messages\n",
        "result = chat_model.invoke(messages)\n",
        "\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76H7Uel3zdIX",
        "outputId": "fe23330b-d549-4335-a23d-986c68eacbe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"A **red blood cell (RBC)**, also known as an **erythrocyte**, is the most common type of blood cell and the principal means of delivering oxygen to the body tissues via the blood.\\n\\nHere's a breakdown of its key characteristics and functions:\\n\\n1.  **Primary Function: Oxygen Transport**\\n    *   Red blood cells pick up oxygen from the lungs.\\n    *   They transport this oxygen to all the tissues and organs throughout the body, which need oxygen to produce energy.\\n    *   They also help transport carbon dioxide (a waste product of metabolism) from the tissues back to the lungs to be exhaled.\\n\\n2.  **Hemoglobin**\\n    *   The distinctive red color of blood comes from **hemoglobin**, an iron-rich protein found inside red blood cells.\\n    *   Hemoglobin is crucial because it's what actually binds to oxygen molecules in the lungs and releases them in the tissues. It also binds to carbon dioxide.\\n\\n3.  **Unique Shape**\\n    *   Red blood cells have a unique **biconcave disc shape** (like a donut with a flattened center, but not a hole).\\n    *   This shape increases their surface area, which is important for efficient gas exchange (picking up and releasing oxygen and carbon dioxide).\\n    *   It also makes them flexible, allowing them to squeeze through tiny blood vessels called capillaries, which are often narrower than the cell itself.\\n\\n4.  **Lack of Nucleus and Organelles**\\n    *   Unlike most other cells in the body, mature red blood cells **lack a nucleus** and other organelles (like mitochondria) in mammals.\\n    *   This adaptation allows them to pack in more hemoglobin, maximizing their oxygen-carrying capacity.\\n    *   However, it also means they cannot repair themselves or reproduce, leading to a relatively short lifespan.\\n\\n5.  **Lifespan and Production**\\n    *   Red blood cells are produced in the **bone marrow**.\\n    *   They have a lifespan of about **100 to 120 days**.\\n    *   Old or damaged red blood cells are removed from circulation, primarily by the spleen and liver.\\n\\nIn summary, red blood cells are highly specialized, anucleated cells packed with hemoglobin, designed to efficiently transport oxygen from the lungs to every part of the body and return carbon dioxide to the lungs for exhalation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--75c64555-7460-4417-b344-9eec6b956448-0', usage_metadata={'input_tokens': 7, 'output_tokens': 1462, 'total_tokens': 1469, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 960}})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_model.invoke(\"What is red blood cell?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi8UoIk6375W"
      },
      "source": [
        "### Second Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJxPJIR-394r",
        "outputId": "f336e1e3-652f-40fe-b548-b4542b699ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I can only answer healthcare-related questions.\n"
          ]
        }
      ],
      "source": [
        "# Same steps with the same SystemMessage, but out of context HumanMessage\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0, google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"Which country is the largest?\"),\n",
        "]\n",
        "\n",
        "result = chat_model.invoke(messages)\n",
        "\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8fFueMP47Bj"
      },
      "source": [
        "### Chat Prompt Template for dynamic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wsUHH1j4_Nv",
        "outputId": "5b1b620a-7929-4d60-f4a9-816630abfc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, someone had a positive experience. They described the discharge process as \"seamless.\"\n"
          ]
        }
      ],
      "source": [
        "# 1. Initializing the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model='gemini-2.5-flash', temperature=0, google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Create the Prompt Template\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions. Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "review_template = ChatPromptTemplate.from_template(instruction_str)\n",
        "\n",
        "# 3. Define the context and question\n",
        "context = \"The discharge process was seamless!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "# 4. Create the chain by piping the components together\n",
        "# Also added an output parser to get a clean string result\n",
        "chain = review_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 5. Invoke the chain with the input variables\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XbbBCQC8zx3"
      },
      "source": [
        "### Using Prompt Templates & Message Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGxaFnH-83c5",
        "outputId": "f9c665be-4a59-4fbb-c63a-6f337741a906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, the patient had a bad experience. The staff was described as \"very rude,\" and the prices were considered \"through the roof.\"\n"
          ]
        }
      ],
      "source": [
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0, google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(input_variables=['context'], template=instruction_str)\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(input_variables=['question'], template=\"{question}\")\n",
        ")\n",
        "\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "# This is the final, reusable prompt template\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "# Define the context and question\n",
        "context = \"The staff was very rude, also the prices are through the roof!\"\n",
        "question = \"Did anyone have a positive or bad experience?\"\n",
        "\n",
        "# Create the chain\n",
        "chain = review_prompt_template | chat_model | StrOutputParser()\n",
        "\n",
        "# Invoke the chain\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QPrYiDFB_mxv",
        "outputId": "1c996d54-52cd-49d6-d29b-8516aa296736"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Yes, one patient stated, \"I had a good stay!\"'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = \"I had a good stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vo_PalZAdS0"
      },
      "source": [
        "### Adding RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "_KaA598KAe9E"
      },
      "outputs": [],
      "source": [
        "REVIEWS_CSV_PATH = \"/content/data/reviews.csv\"\n",
        "\n",
        "# Define variable for directory where the Chroma vector database will be stored\n",
        "REVIEW_CHROMA_PATH = 'chroma_data'\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=REVIEWS_CSV_PATH,\n",
        "    source_column='review'\n",
        ")\n",
        "reviews = loader.load()\n",
        "\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model='models/gemini-embedding-001',\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY')\n",
        ")\n",
        "\n",
        "# Set the size of each batch to process\n",
        "batch_size = 20\n",
        "# Calculate the total number of batches\n",
        "num_batches = (len(reviews) - 1) // batch_size + 1\n",
        "reviews_vector_db = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code uses the `gemini-embedding-001` free model and after processing the data the free quota is exhausted, so later in the code im switching to a local embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "woCnzIZzm6Iy",
        "outputId": "845217e1-f8c4-4e3b-b8bb-508abca485e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1 / 51...\n",
            "Batch 1 processed. Waiting for 30 seconds...\n",
            "Processing batch 2 / 51...\n",
            "Batch 2 processed. Waiting for 30 seconds...\n",
            "Processing batch 3 / 51...\n",
            "Batch 3 processed. Waiting for 30 seconds...\n",
            "Processing batch 4 / 51...\n",
            "Batch 4 processed. Waiting for 30 seconds...\n",
            "Processing batch 5 / 51...\n",
            "Batch 5 processed. Waiting for 30 seconds...\n",
            "Processing batch 6 / 51...\n",
            "Batch 6 processed. Waiting for 30 seconds...\n",
            "Processing batch 7 / 51...\n",
            "Batch 7 processed. Waiting for 30 seconds...\n",
            "Processing batch 8 / 51...\n",
            "Batch 8 processed. Waiting for 30 seconds...\n",
            "Processing batch 9 / 51...\n",
            "Batch 9 processed. Waiting for 30 seconds...\n",
            "Processing batch 10 / 51...\n",
            "Batch 10 processed. Waiting for 30 seconds...\n",
            "Processing batch 11 / 51...\n",
            "Batch 11 processed. Waiting for 30 seconds...\n",
            "Processing batch 12 / 51...\n",
            "Batch 12 processed. Waiting for 30 seconds...\n",
            "Processing batch 13 / 51...\n",
            "Batch 13 processed. Waiting for 30 seconds...\n",
            "Processing batch 14 / 51...\n",
            "Batch 14 processed. Waiting for 30 seconds...\n",
            "Processing batch 15 / 51...\n",
            "Batch 15 processed. Waiting for 30 seconds...\n",
            "Processing batch 16 / 51...\n",
            "Batch 16 processed. Waiting for 30 seconds...\n",
            "Processing batch 17 / 51...\n",
            "Batch 17 processed. Waiting for 30 seconds...\n",
            "Processing batch 18 / 51...\n",
            "Batch 18 processed. Waiting for 30 seconds...\n",
            "Processing batch 19 / 51...\n",
            "Batch 19 processed. Waiting for 30 seconds...\n",
            "Processing batch 20 / 51...\n",
            "Batch 20 processed. Waiting for 30 seconds...\n",
            "Processing batch 21 / 51...\n",
            "Batch 21 processed. Waiting for 30 seconds...\n",
            "Processing batch 22 / 51...\n",
            "Batch 22 processed. Waiting for 30 seconds...\n",
            "Processing batch 23 / 51...\n",
            "Batch 23 processed. Waiting for 30 seconds...\n",
            "Processing batch 24 / 51...\n",
            "Batch 24 processed. Waiting for 30 seconds...\n",
            "Processing batch 25 / 51...\n",
            "Batch 25 processed. Waiting for 30 seconds...\n",
            "Processing batch 26 / 51...\n",
            "Batch 26 processed. Waiting for 30 seconds...\n",
            "Processing batch 27 / 51...\n",
            "Batch 27 processed. Waiting for 30 seconds...\n",
            "Processing batch 28 / 51...\n",
            "Batch 28 processed. Waiting for 30 seconds...\n",
            "Processing batch 29 / 51...\n",
            "Batch 29 processed. Waiting for 30 seconds...\n",
            "Processing batch 30 / 51...\n",
            "Batch 30 processed. Waiting for 30 seconds...\n",
            "Processing batch 31 / 51...\n",
            "Batch 31 processed. Waiting for 30 seconds...\n",
            "Processing batch 32 / 51...\n",
            "Batch 32 processed. Waiting for 30 seconds...\n",
            "Processing batch 33 / 51...\n",
            "Batch 33 processed. Waiting for 30 seconds...\n",
            "Processing batch 34 / 51...\n",
            "Batch 34 processed. Waiting for 30 seconds...\n",
            "Processing batch 35 / 51...\n",
            "Batch 35 processed. Waiting for 30 seconds...\n",
            "Processing batch 36 / 51...\n",
            "Batch 36 processed. Waiting for 30 seconds...\n",
            "Processing batch 37 / 51...\n",
            "Batch 37 processed. Waiting for 30 seconds...\n",
            "Processing batch 38 / 51...\n",
            "Batch 38 processed. Waiting for 30 seconds...\n",
            "Processing batch 39 / 51...\n",
            "Batch 39 processed. Waiting for 30 seconds...\n",
            "Processing batch 40 / 51...\n",
            "Batch 40 processed. Waiting for 30 seconds...\n",
            "Processing batch 41 / 51...\n",
            "Batch 41 processed. Waiting for 30 seconds...\n",
            "Processing batch 42 / 51...\n",
            "Batch 42 processed. Waiting for 30 seconds...\n",
            "Processing batch 43 / 51...\n",
            "Batch 43 processed. Waiting for 30 seconds...\n",
            "Processing batch 44 / 51...\n",
            "Batch 44 processed. Waiting for 30 seconds...\n",
            "Processing batch 45 / 51...\n",
            "Batch 45 processed. Waiting for 30 seconds...\n",
            "Processing batch 46 / 51...\n",
            "Batch 46 processed. Waiting for 30 seconds...\n",
            "Processing batch 47 / 51...\n",
            "Batch 47 processed. Waiting for 30 seconds...\n",
            "Processing batch 48 / 51...\n",
            "Batch 48 processed. Waiting for 30 seconds...\n",
            "Processing batch 49 / 51...\n",
            "Batch 49 processed. Waiting for 30 seconds...\n",
            "Processing batch 50 / 51...\n",
            "Batch 50 processed. Waiting for 30 seconds...\n",
            "Processing batch 51 / 51...\n",
            "Batch 51 processed. Waiting for 30 seconds...\n",
            "Vector database created successfully and saved to the specified directory.\n"
          ]
        }
      ],
      "source": [
        "# Loop through the document in batches to avoid hitting the API`s rate limit.\n",
        "for i in range(0, len(reviews), batch_size):\n",
        "    batch_docs = reviews[i:i+batch_size]\n",
        "    current_batch_num = i // batch_size + 1\n",
        "\n",
        "    print(f\"Processing batch {current_batch_num} / {num_batches}...\")\n",
        "\n",
        "    if i==0:\n",
        "        reviews_vector_db = Chroma.from_documents(\n",
        "            documents=batch_docs,\n",
        "            embedding=embedding_function,\n",
        "            persist_directory=REVIEW_CHROMA_PATH\n",
        "        )\n",
        "    else:\n",
        "        reviews_vector_db.add_documents(documents=batch_docs)\n",
        "\n",
        "    # Pause the script for 30 seconds after each batch to respect the per-minute rate limit.\n",
        "    print(f\"Batch {current_batch_num} processed. Waiting for 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"Vector database created successfully and saved to the specified directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Switching the embedding model to local `all-mpnet-base-v2` for free embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580,
          "referenced_widgets": [
            "d4f6f00f92c6472ea5e5f2357245ad4d",
            "ea1a1e524b4e42e7aac6fdf2a02f74df",
            "0ebbebfb002841888a95900c3c8314a7",
            "103ec38bd60d468492c754ff6b9a08f9",
            "adc5bd96d99147be94e13cfcd3b45fbe",
            "546530ee6a554021a0083714676e9dc6",
            "2834e7699b924606a0c483f5b9251e1d",
            "68821cac80a74256a475d394b5e18063",
            "a0ddbb7d6f5149f19c325f3286105fa4",
            "dfed89cb30d74fe8936c906843a576f8",
            "023532b376c5415181ef31d235129f70",
            "8561df89bbe04844b13b9197b3b5db30",
            "25bc3d50dc1d459b95dfd3213248aa06",
            "4cb39dda4fba4ad0a0252e398cbd5b87",
            "bc9efdc90f444f6798a5f35c3aae5299",
            "7eac68c9d67c479ab951acecd4624471",
            "46231f50e3ba449aab9b559a3eaa6640",
            "cb8311efabf245d4a7ff877fc8224bef",
            "07c0c9893901489b9e719bd0ec452fb4",
            "04998777c0eb4ed282cb944c3b1fb93e",
            "ecd13b0fe71a49a280dc0ec3fbe7813c",
            "ff262c88162143cda79eb35768d69394",
            "dd4e2024a09743989ae1d9741a9ec672",
            "45c9a6d6819a4f1eaddc409a25bb364e",
            "411d30c8f5674660a81144b5b398ca94",
            "e509c49685154282833b3fae133075c4",
            "044ebb8d8d1d4f1782148232334469ab",
            "1b86501a71dd4c23bc2fd9467bcf28c2",
            "a174c919aaab4a6cbb885e16a6171e3f",
            "e6e81aa07ed84dddbf3b536bc667fc68",
            "0f6ac959c39243b2bc8676612a0f51e5",
            "3178b239e580491b881b7e30ade05f0b",
            "5db9bf85a0944c6ba3677c8de37755a2",
            "cf728e07ffe746a9af12af100052c9c7",
            "a1913396bcc24f3bb69865b06d620788",
            "90feedaddd6143e9a39e4785b70d345a",
            "fcd382f3b8d64035a3708d4a817e73be",
            "bece3624ac0642ecbf967e4f28889dd1",
            "3a7facdfea40433d916a0cca81b57dc2",
            "4023202386f340c386fbee30d1e3efc7",
            "96b3f17a8d38492ba317c188b29e84e8",
            "dd2f97761035461e8db85331763e7c88",
            "f775d28069494918893aa294c3d5cfd3",
            "0223178518c54461aef02e5acabe8f11",
            "fb5e9b29bafe48669c2945f93fdcfc5f",
            "7adb68b8840e4f789cdd919c60365894",
            "d625f14fd538401c881fd89070af0122",
            "fdc3a14123ea431b92b9aa7c10f4371a",
            "c772eb3f69374d1986a5bc7fd6a88643",
            "bef71a58232c475cacc0880a22d93247",
            "ddcf65578aa74668a572b2aea54a5278",
            "81dc9576b6f04f9d9cbc9000632d8ce2",
            "081b08362eb04c6db09c072fca1ec3e3",
            "99c7871ea0e24aa294e063e8dbedda15",
            "b7dfd3a58f5c4bfab025fee24d3f1433",
            "656de4e4616642d5a1cd0396ce3606be",
            "2a996a679eb24e04984c27971a953d5a",
            "dfec41aed9bf4d8eb94ccbe0572681e8",
            "00fc94688d7c422b8234d5262d06574f",
            "1d4465f8c8254e1ea64d03273a482791",
            "49fad9fabe264871b50c4d8be464c786",
            "8c9cc0d49dda4404b036cba6048d8862",
            "221a6e12a3e14a87abfd47d2f1f443bf",
            "e6118d0a423e4d6ebb5fb6e4b3bd853a",
            "32045d4800e342929d63f51350e6eb02",
            "b5b09aa1464c48b8832df58c11bd303d",
            "c09e3a4dc51c4665b4c5837200232b8a",
            "887570fc41774d92b139224206849939",
            "d5db412a51414f3fb728d2fbf6463790",
            "6f165790a5b54d64b9d6dff2abe688c1",
            "555631e466e54da08c50fd6103b56159",
            "335abab5c70c47e491ae7192680fb5a0",
            "d2c83176a67b46b191be27af7e257c9e",
            "f446e6bf12e7450c93bd0357e635874e",
            "68f08ef976424a0b84f081e9dd5ddaaf",
            "bd594b8f840840cc8d147065614324af",
            "5c9bc68ad2ca40309ef8173178b200cf",
            "0f278a08ae86488aa5eb4293d6c1638c",
            "6c2120ff1fea473b88444a3990c9b56f",
            "6c2766916e12439f9a1c9dbc5a98b9d2",
            "507189efa65848dd905f8f4adb0ac1b4",
            "51f0f537ca5442abbd8ee2811e1c542b",
            "43171d503341409aac0356f25d5e35bc",
            "a3fd2c88cee7497c99bdb8c8f42125ec",
            "c667e3f35d2e48b19f1e9b08b580689a",
            "4b96b0254d0d4a7b8d3b9d8b4ee535b4",
            "82ee7b236960405bac5e2c84fc34022b",
            "fc44d18f563f45c39c7f8c55b8ca2f13",
            "9ba88ff43bbf4350972e5d2549c39c4c",
            "9acf60eee5b14ee9b31793dcf7742a05",
            "cddf2972681f4eca85601ac85a305e9c",
            "c5207d26b0a046309f489c11d53f3c72",
            "658bba2fda4348ffbe35562a56eb15d9",
            "bb81a2347a724ae88657dde23e050c76",
            "9a553bb7d6f54a19bdbd285376810223",
            "ca91ba4bde94462c87d12492ab0c8bf7",
            "c48a3daffc384abb9e233063196bab78",
            "3cfde60dda3845a98f492099a3194226",
            "8d547115464341d1be90a8d872f6682c",
            "3ed0efae7d0a438c8b338af4cf19d077",
            "66ac4651a083441290f4ecf5b6a1a800",
            "d75c89330b6e4fab91257865bfcca791",
            "ee4659d5e8f941ec87f308350bf422fc",
            "3663d2b27fb04b02b7e72a21ae1480f1",
            "d374a92e56e14f2ba8df74f1c675c127",
            "9a1063aacc524eb9a68c782d2d8f496d",
            "bb84750ad6cc499aa8c1aed1fede1bf8",
            "b1a8af76754444fdb24b9e04c114e806",
            "dcfc3e1bfe2645bcbb63d8cbf54f0119",
            "e27c0b6c9aea40b58f4d0513894c7028",
            "08a403326ecb4f8fa8e96ea7fbf9a8f7",
            "06058e8dfaa7440089311a946b2d9d26",
            "d74ed129b9254e53a38cc4308d481574",
            "0157710a42db4406be08ae3404ffac72",
            "bcd34e858b1f48e18c51bdd192f24b9b",
            "2fb05a0910514cde9e96e789023e47f3",
            "f1ea473a4a2e4d76a8ca9f2b573065b6",
            "a141272cc650445eb78407e104dc0b4f",
            "5b31ff10cb924802a634f9a950403e7f",
            "65be7f59dca84a4698de89e92ed0e6a8",
            "ad1f17b2f6a34098ac03401aff5ca053"
          ]
        },
        "id": "H2I371gkze9u",
        "outputId": "07f1e6fa-f11d-4d7b-f273-98049a514203"
      },
      "outputs": [],
      "source": [
        "# Load CSV\n",
        "df = pd.read_csv(\"/content/data/reviews.csv\")\n",
        "reviews_docs = [Document(page_content=text) for text in df['review']]\n",
        "\n",
        "# Load SentenceTransformer model\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "# Create Chroma DB\n",
        "start = time.time()\n",
        "\n",
        "print(\"Creating the Database...\")\n",
        "reviews_vector_db = Chroma.from_documents(\n",
        "    documents=reviews_docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"/content/chroma_db_mpnet\"\n",
        ")\n",
        "\n",
        "print(f\"Time taken: {time.time() - start} seconds.\")\n",
        "print(\"Vector database created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGYfwI22mSQP"
      },
      "source": [
        "### Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVlV_UVcJz3h"
      },
      "source": [
        "`similarity_search` function does the embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmcfJyTrmUb0",
        "outputId": "e1da250c-31ad-46d6-b61e-277b03d39103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : I encountered some communication issues during my stay. The medical staff seemed disorganized, and it led to confusion about my treatment plan.\n",
            "2 : I encountered some issues with the nursing staff's communication. It seemed like there was a lack of coordination, leading to confusion about my medication schedule and treatment plan.\n",
            "3 : The hospital staff were friendly and attentive, making my stay more pleasant. However, there were occasional lapses in communication that caused confusion about my treatment plan.\n"
          ]
        }
      ],
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "relevant_chunks = reviews_vector_db.similarity_search(question, k=3)\n",
        "\n",
        "for i, review in enumerate(relevant_chunks):\n",
        "    print(i+1, \":\", review.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U-4_bQOrLsA6"
      },
      "outputs": [],
      "source": [
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "\n",
        "# Create a chain for querying and generating responses\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    | review_prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tl1MwrHJNNgR",
        "outputId": "2a23fea4-d95e-4672-d087-6ac199c21a44"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Yes, several patients have complained about communication with the hospital staff.\\n\\nSpecifically:\\n*   One patient encountered communication issues during their stay, noting that the medical staff seemed disorganized, which led to confusion about their treatment plan.\\n*   Another patient experienced issues with the nursing staff's communication, citing a lack of coordination that caused confusion about their medication schedule and treatment plan.\\n*   Two separate reviews mentioned that while the hospital staff were friendly and attentive, there were occasional lapses in communication that caused confusion about their treatment plan.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "review_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4J-95-PQltI"
      },
      "source": [
        "### Adding a UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxkLSFVkQrIf"
      },
      "outputs": [],
      "source": [
        "def respond_to_user_question(question):\n",
        "    \"\"\"\n",
        "    Respond to a user`s question using the review_chain.\n",
        "    \"\"\"\n",
        "    return review_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the chat model `gemini-2.5-flash` with gradio UI. (the output can not be rendered in the browser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "9yGVx3HBRZ7C",
        "outputId": "23d7863e-e2a6-4697-f22d-8b99c04ad7d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4acf096688f5242bd3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4acf096688f5242bd3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4acf096688f5242bd3.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interface = gr.ChatInterface(fn=respond_to_user_question, title=\"Review Helper Bot\")\n",
        "\n",
        "interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiPduv_PZYlf"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/phi3_128k\"\n",
        "\n",
        "# Create folder\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "!hf download bartowski/Phi-3.1-mini-128k-instruct-GGUF \\\n",
        "    --include \"Phi-3.1-mini-128k-instruct-Q4_K_M.gguf\" \\\n",
        "    --local-dir \"$model_path\"\n",
        "\n",
        "print(\"Download done, file at:\", os.listdir(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also switching the chat model from `gemini-2.5-flash` to `Phi-3.1-mini-128k-instruct-Q4_K_M` because the free version of `gemini` has limited tokens. `Phi-3.1-mini` is a local model that i've saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENsJM_NPZ7GL"
      },
      "outputs": [],
      "source": [
        "chat_model = LlamaCpp(\n",
        "    model_path=\"/content/phi3_128k/Phi-3.1-mini-128k-instruct-Q4_K_M.gguf\",\n",
        "    n_ctx=8096,          # context window changed form 128k to 8k because free tier of colab is limiter on RAM\n",
        "    temperature=0.0,\n",
        "    max_tokens=512\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPldgRASf3xa"
      },
      "outputs": [],
      "source": [
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    | review_prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing `Phi-3.1-mini` (not rendable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "_b4lgbJ8gCM6",
        "outputId": "bd3d1e56-63cd-4035-f9f1-b9f06e45807d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e658227459ea7781bf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e658227459ea7781bf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e658227459ea7781bf.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interface = gr.ChatInterface(fn=respond_to_user_question, title=\"Review Helper Bot\")\n",
        "\n",
        "interface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
